<!DOCTYPE html>
<html>
  <head>
    <!-- Google Automatic Advertising -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4525414114581084"
     crossorigin="anonymous"></script>

<!-- meta --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1.0,minimum-scale=1.0">
<link rel="icon" type="image/x-icon" href="/images/logo.png">
<link rel="stylesheet" type="text/css" href="/css/common.css">

<script type="text/javascript" src="/js/jquery-1.9.0.min.js"></script>
<script type="text/javascript" src="/js/google-code-prettify/prettify.js"></script>
<link  type="text/css" rel="stylesheet" href="/js/google-code-prettify/prettify.css"/>
<script>
  $(function(){
    $("pre").addClass("prettyprint");
    function init(event){
      prettyPrint();
    }
    if(window.addEventListener)window.addEventListener("load",init,false);
    else if(window.attachEvent)window.attachEvent("onload",init);
    
    $(".to-top").click(function() {
      $('body, html').animate({scrollTop: 0}, 300, 'linear');;
    });
  });
</script>

<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-5YWD0EFVYR"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-5YWD0EFVYR');
</script>

<title>Entries - Perl AI Deep Learning Tutorial</title>
<meta name="description" content="Entries of Perl AI Deep Learning Tutorial">
  </head>
  <body>
    <div class="container">
      <div class="header">
        <div class="header_main">
  <h1>
    <a href="/"><img src="/images/logo.png">Perl AI Deep Learning Tutorial</a>
  </h1>
</div>

      </div>
      <div class="main">
        <div class="content">
          <div class="entry">
  <div class="top">
    <!-- top -->
  </div>
  <div class="middle">
    <h2><a href="/list.html">Entries</a></h2>
<ul>
  <li style="list-style:none;">
    <b>2021</b>
  </li>
  <li style="list-style:none">
    9/20 <a href="/blog/20210920074017.html"> What is deep learning? </a>
  </li>
  <li style="list-style:none">
    7/21 <a href="/blog/20210721085555.html"> The 2021 AI programming course for middle and high school students has started </a>
  </li>
  <li style="list-style:none">
    5/10 <a href="/blog/20210510142804.html">Deep Learning using SPVM</a>
  </li>
  <li style="list-style:none;">
    <b>2020</b>
  </li>
  <li style="list-style:none">
    11/18 <a href="/blog/20201118093000.html"> Perl's Deep Learning Library-AI::MXNet </a>
  </li>
  <li style="list-style:none">
    10/26 <a href="/blog/20201026095954.html"> Inverse error propagation method --Start writing backpropagation </a>
  </li>
  <li style="list-style:none">
    10/23 <a href="/blog/20201023083657.html"> What is the parameter update optimization algorithm? </a>
  </li>
  <li style="list-style:none">
    10/22 <a href="/blog/20201022083657.html"> Adam --Parameter update optimization algorithm with improved SGD </a>
  </li>
  <li style="list-style:none">
    10/21 <a href="/blog/20201021085550.html"> How to find the slope in the case of a multi-stage function --Differentiation of the composite function </a>
  </li>
  <li style="list-style:none">
    10/20 <a href="/blog/20201020085300.html"> What is slope? --Differential coefficient </a>
  </li>
  <li style="list-style:none">
    10/19 <a href="/blog/20201019123741.html"> tanh function --activation function </a>
  </li>
  <li style="list-style:none">
    10/18 <a href="/blog/20201018123741.html"> Derivative of tanh function --Derivative of activation function </a>
  </li>
  <li style="list-style:none">
    10/17 <a href="/blog/20201017123741.html"> Stochastic Gradient Descent (SGD)-Update Weight and Bias Parameters </a>
  </li>
  <li style="list-style:none">
    10/16 <a href="/blog/20201016143424.html"> What is bias? </a>
  </li>
  <li style="list-style:none">
    10/15 <a href="/blog/20201015143424.html"> What is a weight? </a>
  </li>
  <li style="list-style:none">
    10/7 <a href="/blog/20201007144439.html"> Initial value of Xavier </a>
  </li>
  <li style="list-style:none">
    10/6 <a href="/blog/20201006144439.html"> Initial value of He </a>
  </li>
  <li style="list-style:none">
    10/5 <a href="/blog/20201005144439.html"> Find random numbers that follow a normal distribution </a>
  </li>
  <li style="list-style:none">
    10/2 <a href="/blog/20201002161518.html"> softmax function </a>
  </li>
  <li style="list-style:none">
    10/1 <a href="/blog/20201001161518.html"> Derivative of ReLU function --Derivative of activation function </a>
  </li>
  <li style="list-style:none">
    9/29 <a href="/blog/20200929161518.html"> Create a column-first matrix </a>
  </li>
  <li style="list-style:none">
    9/28 <a href="/blog/20200928161518.html"> What is a matrix? </a>
  </li>
  <li style="list-style:none">
    9/27 <a href="/blog/20200927161518.html"> Derivative of the softmax function and the composite function of the cross entropy error </a>
  </li>
  <li style="list-style:none">
    9/26 <a href="/blog/20200926161518.html"> MNIST handwriting recognition deep learning written in pure Perl </a>
  </li>
  <li style="list-style:none">
    9/24 <a href="/blog/20200924123308.html"> Calculation of vector inner product </a>
  </li>
  <li style="list-style:none">
    9/23 <a href="/blog/20200923123308.html"> What is the learning rate? </a>
  </li>
  <li style="list-style:none">
    9/22 <a href="/blog/20200922123308.html"> Reverse error propagation method-Reverse error propagation method </a>
  </li>
  <li style="list-style:none">
    9/21 <a href="/blog/20200921123308.html"> What is a gradient? </a>
  </li>
  <li style="list-style:none">
    9/20 <a href="/blog/20200920123308.html"> Derivative of sigmoid function </a>
  </li>
  <li style="list-style:none">
    9/19 <a href="/blog/20200919123308.html"> What is a derivative? </a>
  </li>
  <li style="list-style:none">
    9/17 <a href="/blog/20200917123308.html"> Find the transposed matrix </a>
  </li>
  <li style="list-style:none">
    9/16 <a href="/blog/20200916101844.html"> Express the expected output with probability </a>
  </li>
  <li style="list-style:none">
    9/15 <a href="/blog/20200915121719.html"> Calculation process to get the final output from the initial input by deep learning </a>
  </li>
  <li style="list-style:none">
    9/14 <a href="/blog/20200914103640.html"> Matrix product calculation </a>
  </li>
  <li style="list-style:none">
    9/13 <a href="/blog/20200913103640.html"> Find the sum of vectors </a>
  </li>
  <li style="list-style:none">
    9/12 <a href="/blog/20200912123308.html"> Find the sum of matrices </a>
  </li>
  <li style="list-style:none">
    9/11 <a href="/blog/20200911102242.html"> ReLU function --activation function </a>
  </li>
  <li style="list-style:none">
    9/10 <a href="/blog/20200910120907.html"> Find the sum of squares error-loss function </a>
  </li>
  <li style="list-style:none">
    9/9 <a href="/blog/20200909120907.html"> Read MNIST label information </a>
  </li>
  <li style="list-style:none">
    9/8 <a href="/blog/20200908120907.html"> Handwriting recognition with Perl + deep learning </a>
  </li>
  <li style="list-style:none">
    9/7 <a href="/blog/20200907120907.html"> Read MNIST image information </a>
  </li>
  <li style="list-style:none">
    9/6 <a href="/blog/20200906120907.html"> Randomly shuffle training data </a>
  </li>
  <li style="list-style:none">
    9/5 <a href="/blog/20200905120907.html"> Representing the number of neurons in the hidden layer </a>
  </li>
  <li style="list-style:none">
    9/4 <a href="/blog/20200904120907.html"> The unit called epoch refers to going around the training data </a>
  </li>
  <li style="list-style:none">
    9/3 <a href="/blog/20200903120907.html"> Sigmoid function-activation function </a>
  </li>
  <li style="list-style:none">
    9/2 <a href="/blog/20200902120907.html"> What is the activation function? </a>
  </li>
  <li style="list-style:none">
    9/1 <a href="/blog/20200901120907.html"> What is a loss function? </a>
  </li>
  <li style="list-style:none">
    8/31 <a href="/blog/20200831120907.html"> Find the cross entropy error-loss function </a>
  </li>
  <li style="list-style:none">
    8/30 <a href="/blog/20200830120907.html"> What is batch size?-Online learning, mini-batch learning, batch learning </a>
  </li>
  <li style="list-style:none">
    8/29 <a href="/blog/20200829120907.html"> Find the difference in the matrix </a>
  </li>
  <li style="list-style:none">
    8/28 <a href="/blog/20200828120907.html"> Find the difference between the vectors </a>
  </li>
  <li style="list-style:none">
    3/11 <a href="/blog/20200311113241.html"> How to set the initial values ​​of the weight and bias parameters of each layer </a>
  </li>
  <li style="list-style:none">
    3/6 <a href="/blog/20200306113052.html"> Calculation in the output layer </a>
  </li>
  <li style="list-style:none">
    3/2 <a href="/blog/20200302113052.html"> Computations in the middle layer-convert m inputs to n outputs </a>
  </li>
</ul>

  </div>
  <div class="bottom">
    <h3>Associated Information</h3>

<div style="margin:10px 0">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4525414114581084"
     crossorigin="anonymous"></script>
<ins class="adsbygoogle"
     style="display:block"
     data-ad-format="autorelaxed"
     data-ad-client="ca-pub-4525414114581084"
     data-ad-slot="9163995495"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
</div>

  </div>
</div>

        </div>
        <div class="side">
          
        </div>
      </div>
      <div class="footer">
        <div class="perlri_link">
  <a rel="nofollow" href="https://perlclub.net/en">Perl Club</a>
</div>

      </div>
    </div>
  </body>
</html>
