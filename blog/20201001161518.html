<!DOCTYPE html>
<html>
  <head>
    <!-- Google Automatic Advertising -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4525414114581084"
     crossorigin="anonymous"></script>

<!-- meta --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1.0,minimum-scale=1.0">
<link rel="icon" type="image/x-icon" href="/images/logo.png">
<link rel="stylesheet" type="text/css" href="/css/common.css">

<script type="text/javascript" src="/js/jquery-1.9.0.min.js"></script>
<script type="text/javascript" src="/js/google-code-prettify/prettify.js"></script>
<link  type="text/css" rel="stylesheet" href="/js/google-code-prettify/prettify.css"/>
<script>
  $(function(){
    $("pre").addClass("prettyprint");
    function init(event){
      prettyPrint();
    }
    if(window.addEventListener)window.addEventListener("load",init,false);
    else if(window.attachEvent)window.attachEvent("onload",init);
    
    $(".to-top").click(function() {
      $('body, html').animate({scrollTop: 0}, 300, 'linear');;
    });
  });
</script>

<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-5YWD0EFVYR"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-5YWD0EFVYR');
</script>

<title> Derivative of ReLU function --Derivative of activation function  - Perl AI Deep Learning Tutorial</title>
<meta name="description" content="Let's write the  derivative of the  ReLU function in Perl.">
  </head>
  <body>
    <div class="container">
      <div class="header">
        <div class="header_main">
  <h1>
    <a href="/"><img src="/images/logo.png">Perl AI Deep Learning Tutorial</a>
  </h1>
</div>

      </div>
      <div class="main">
        <div class="content">
          <div class="entry">
  <div class="top">
    <!-- top -->
  </div>
  <div class="middle">
    <h2><a href="/blog/20201001161518.html"> Derivative of ReLU function --Derivative of activation function </a></h2>
<p>
  Let's write the <a href="/blog/20200919123308.html"> derivative</a> of the <a href="/blog/20200911102242.html"> ReLU function</a> in Perl.
</p>
<div style="width:calc(100% - 30px);margin:10px auto;">
  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4525414114581084"
       crossorigin="anonymous"></script>
  <!-- 最初の段落下 - ディスプレイ 横長 レスポンシブ -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-4525414114581084"
       data-ad-slot="2828858335"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


<p>
  Let's write the derivative of the ReLU function in Perl. The derivative of the ReLU function is used for the reverse mispropagation method.
</p>
<pre>
use strict;
use warnings;

sub relu_derivative {
   my ($x) = @_;
  
   my $relu_derivative = 1 * ($x&gt; 0.0);
  
   return $relu_derivative;
}

my $value1 = 0.7;
my $relu_derivative1 = relu_derivative ($value1);

print "$relu_derivative1\n";

my $value2 = -0.4;
my $relu_derivative2 = relu_derivative ($value2);

print "$relu_derivative2\n";
</pre>th, $columns_length) = @_;
  
  my $values_length = $rows_length * $columns_length;
  my $mat = {
    rows_length => $rows_length,
    columns_length => $columns_length,
    values ​​=> [(0) x $values_length],
  };;
  
  return $mat;
<p>
  }
</p>
<p>
  # Find the product of matrices
</p>
<p>
  sub mat_mul {
</p>
  my ($mat1, $mat2) = @_;
  
  my $mat1_rows_length = $mat1->{rows_length};
  my $mat1_columns_length = $mat1->{columns_length};
  my $mat1_values ​​= $mat1->{values};
  
  my $mat2_rows_length = $mat2->{rows_length};
  my $mat2_columns_length = $mat2->{columns_length};
  my $mat2_values ​​= $mat2->{values};
  
  #Calculation of matrix product
  my $mat_out_values ​​= [];
  for (my $row = 0; $row <$mat1_rows_length; $row ++) {
    for (my $col = 0; $col <$mat2_columns_length; $col ++) {
      for (my $incol = 0; $incol <$mat1_columns_length; $incol ++) {
        $mat_out_values->[$row + $col * $mat1_rows_length]
         + = $mat1_values->[$row + $incol * $mat1_rows_length] * $mat2_values->[$incol + $col * $mat2_rows_length];
      }
    }
  }
  
  my $mat_out = {
    rows_length => $mat1_rows_length,
    columns_length => $mat2_columns_length,
    values ​​=> $mat_out_values,
  };;
  
  return $mat_out;
<p>
  }
</p>
<p>
  #Creating a column-first matrix
</p>
<p>
  sub mat_new {
</p>
  my ($values, $rows_length, $columns_length) = @_;
  
  my $mat = {
    rows_length => $rows_length,
    columns_length => $columns_length,
    values ​​=> $values,
  };;
  
  return $mat;
<p>
  }
</p>
<p>
  # Transpose matrix (replace matrix)
</p>
<p>
  sub mat_transpose {
</p>
  my ($mat) = @_;
  
  my $rows_length = $mat->{rows_length};
  my $columns_length = $mat->{columns_length};
  my $length = $rows_length * $columns_length;
  
  my $mat_trans = {};
  $mat_trans->{rows_length} = $columns_length;
  $mat_trans->{columns_length} = $rows_length;
  
  my $values ​​= $mat->{values};
  my $mat_trans_values ​​= [];
  
  for (my $row_index = 0; $row_index <$rows_length; $row_index ++) {
    for (my $column_index = 0; $column_index <$columns_length; $column_index ++) {
      $mat_trans_values->[$row_index * $columns_length + $column_index] = $values->[$column_index * $rows_length + $row_index];
    }
  }
  $mat_trans->{values} = $mat_trans_values;
  
  return $mat_trans;
<p>
  }
</p>
<p>
  sub array_div_scalar {
</p>
  my ($nums, $scalar_num) = @_;
  
  my $nums_out = [];
  for (my $i = 0; $i <@$nums; $i ++) {
    $nums_out->[$i] = $nums->[$i] / $scalar_num;
  }
  
  return $nums_out;
<p>
  }
</p>
<p>
  #softmax function
</p>
<p>
  sub softmax {
</p>
  my ($nums) = @_;
  
  my $exp_total = 0;
  for (my $i = 0; $i <@$nums; $i ++) {
    $exp_total + = exp($nums->[$i]);
  }
  
  my $nums_out = [];
  for (my $i = 0; $i <@$nums; $i ++) {
    $nums_out->[$i] = exp($nums->[$i]) / $exp_total;
  }
  
  return $nums_out;
<p>
  }
</p>
<p>
  #softmax Derivative of cross entropy error
</p>
<p>
  sub softmax_cross_entropy_cost_derivative {
</p>
  my ($softmax_outputs, $desired_outputs) = @_;
  
  my $length = @$softmax_outputs;
  
  my $softmax_cross_entropy_cost_derivative = [];
  for (my $i = 0; $i <@$softmax_outputs; $i ++) {
    $softmax_cross_entropy_cost_derivative->[$i] = ($softmax_outputs->[$i]-$desired_outputs->[$i]) / $length;
  }
  
  return $softmax_cross_entropy_cost_derivative;
<p>
  }
</p>
</pre>

  </div>
  <div class="bottom">
    <h3>Associated Information</h3>

<div style="margin:10px 0">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4525414114581084"
     crossorigin="anonymous"></script>
<ins class="adsbygoogle"
     style="display:block"
     data-ad-format="autorelaxed"
     data-ad-client="ca-pub-4525414114581084"
     data-ad-slot="9163995495"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
</div>

  </div>
</div>

        </div>
        <div class="side">
          
        </div>
      </div>
      <div class="footer">
        <div class="perlri_link">
  <a rel="nofollow" href="https://perlclub.net/en">Perl Club</a>
</div>

      </div>
    </div>
  </body>
</html>
