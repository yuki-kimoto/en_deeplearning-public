<!DOCTYPE html>
<html>
  <head>
    <!-- Google Automatic Advertising -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4525414114581084"
     crossorigin="anonymous"></script>

<!-- meta --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1.0,minimum-scale=1.0">
<link rel="icon" type="image/x-icon" href="/images/logo.png">
<link rel="stylesheet" type="text/css" href="/css/common.css">

<script type="text/javascript" src="/js/jquery-1.9.0.min.js"></script>
<script type="text/javascript" src="/js/google-code-prettify/prettify.js"></script>
<link  type="text/css" rel="stylesheet" href="/js/google-code-prettify/prettify.css"/>
<script>
  $(function(){
    $("pre").addClass("prettyprint");
    function init(event){
      prettyPrint();
    }
    if(window.addEventListener)window.addEventListener("load",init,false);
    else if(window.attachEvent)window.attachEvent("onload",init);
    
    $(".to-top").click(function() {
      $('body, html').animate({scrollTop: 0}, 300, 'linear');;
    });
  });
</script>

<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-5YWD0EFVYR"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-5YWD0EFVYR');
</script>

<title> Reverse error propagation method-Reverse error propagation method  - Perl AI Deep Learning Tutorial</title>
<meta name="description" content="I will explain the algorithm that is the most difficult to understand in deep learning, the reverse mispropagation method (reverse error propagation method).">
  </head>
  <body>
    <div class="container">
      <div class="header">
        <div class="header_main">
  <h1>
    <a href="/"><img src="/images/logo.png">Perl AI Deep Learning Tutorial</a>
  </h1>
</div>

      </div>
      <div class="main">
        <div class="content">
          <div class="entry">
  <div class="top">
    <!-- top -->
  </div>
  <div class="middle">
    <h2><a href="/blog/20200922123308.html"> Reverse error propagation method-Reverse error propagation method </a></h2>
<p>
  I will explain the algorithm that is the most difficult to understand in deep learning, the reverse mispropagation method (reverse error propagation method).
</p>
<div style="width:calc(100% - 30px);margin:10px auto;">
  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4525414114581084"
       crossorigin="anonymous"></script>
  <!-- 最初の段落下 - ディスプレイ 横長 レスポンシブ -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-4525414114581084"
       data-ad-slot="2828858335"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


<h3> What is the reverse mispropagation method? </H3>
<p>
  The reverse mispropagation method is one of the algorithms to find the slope of the loss function with respect to the weight and bias parameters of each layer.
</p>
<p>
  There is also an easier-to-understand algorithm than the reverse mispropagation method for determining the slope of the loss function for each layer's weight and bias parameters.
</p>
<p>
  In deep learning, performance is important, so think of the sample code as having the reverse mispropagation method from the beginning.
</p>
<p>
  Let's consider the reverse mispropagation method by dividing it into several elements.
</p>
<h3> Find the slope of the loss function for the individual weight and bias parameters of the final layer </h3>
<p>
  The reverse mispropagation method goes from behind. The final output is the loss function. The loss function is an indicator of error.
</p>
<p>
  First, consider the relationship between the bias parameter of the last layer and the loss function. Think of weights and biases as being defined. Think of vec_add as the product of vectors, mat_mul as the matrix product, and vec_sub as the function to find the difference between the vectors.
</p>
<pre>
# weight
my $weights = [..., ..., ..., ..., ..., ..., ..., ..., ..., ..., ...,. ..,];

#Bias
my $biases = [..., ..., ...];

#Expected output
my $desired_outputs = [1, 0, 0];

# input
my $inputs = [0.3, 0.2, 0, 0.5];

# output
my $outputs = vec_add (mat_mul ($wieghts, $inputs), $biases);

# Activated output
my $activate_outputs = activate ($outputs);

# Loss function result (error index)
my $cost = cost (vec_sub ($desired_outputs, $activate_outputs);
</pre>
<p>
  Now suppose you want to see how the loss function works when you move one value of the bias. For example, suppose that when you increase Bias 1 by a small value of 0.001, the loss function decreases by 0.002.
</p>
<p>
  At this time, the slope of the loss function with respect to bias 1 is "-2 = -0.002 / 0.001". Then, in turn, find all the biases and all the weights. The reverse mispropagation method is an algorithm for finding this value quickly.
</p>
<p>
  From here on, I don't think about difficult things anymore. As a software engineer, let's think that this is what you want. We will implement the method derived by AI researchers with mathematical knowledge.
</p>
<h4> Slope of loss function with respect to bias </h4>
<p>
  First, find the slope of the loss function with respect to the first bias.
</p>
<pre>
# Slope with respect to the loss function of one bias
# = Derivative of activation function (1 output) * Derivative of loss function (1 expected output, 1 activated output);
my $bias_grads0 = activate_derivative ($outputs-&gt;[0]) * cost_derivative ($desired_outputs-&gt;[0], $activate_outputs-&gt;[0]);
</pre>
<p>
  Since we want to find the slope of the loss function for all biases, we loop for.
</p>
<pre>
my $biase_grads = [];
for (my $i = 0; $i &lt;@$biases; $i ++) {
  $bias_grads-&gt;[$i] = activate_derivative ($outputs-&gt;[0]) * cost_derivative ($desired_outputs-&gt;[0], $activate_outputs-&gt;[0]);
}
</pre>
<h4> Slope of loss function with respect to weight </h4>
<p>
  Find the slope of the loss function with respect to the weight.
</p>
<pre>
# Loss function slope for one weight 0 rows 0 columns = one bias slope found above 0 * one input 0
my $weight_grads0 = $bias_grads-&gt;[0] * $inputs-&gt;[0];
</pre>
<p>
  The weights are represented as a column-major matrix, so the for loop looks like this:
</p>
<pre>
my $weights_grads = [];
for (my $input_index = 0; $input_index &lt;$inputs_length; $input_index ++) {
  for (my $bias_index = 0; $bias_index &lt;$bias_length; $bias_index ++) {
    my $weight_grad_index = $bias_index + ($input_index * $biases_length);
    $weight_grads-&gt;[$weight_grad_index] = $biase_grads-&gt;[$biase_index] * $inputs-&gt;[$input_index];
  }
}
</pre>
<p>
  This can also be expressed as a matrix multiplication of the bias as a vertical vector and the input as a horizontal vector.
</p>
<pre>
#Bias slope
1
2
3

# input
4 5 6 7

#Slope of loss function with respect to weight
4 5 6 7
8 10 12 14
12 15 18 21
</pre>
<p>
  The calculation content on this page is currently undergoing trial and error.
</p>
<h3> Find the slope of the loss function for the individual weight and bias parameters of the middle layer </h3>
<p>
  In the reverse mispropagation method, the layer is traced backwards to find the slope of the weight and bias, but the slope of the bias found above is used.
</p>
<pre>
# weight
my $weights = [..., ..., ..., ..., ..., ..., ..., ..., ..., ..., ...,. ..,];

#Bias
my $biases = [..., ..., ...];

# input
my $inputs = [0.3, 0.2, 0, 0.5];

# output
my $outputs = vec_add (mat_mul ($wieghts, $inputs), $biases);

# Activated output
my $activate_outputs = activate ($outputs);
</pre>
<h4> Slope of loss function with respect to bias </h4>
<pre>
Loss function slope with respect to bias 0 = Inner product of "slope of next layer bias 0-n" and "weight of next layer 0 columns 0-n rows"
Slope of loss function with respect to bias 1 = Dot product of "slope of bias of next layer 0 to n" and "weight of next layer 1 column 0 to n rows"
</pre>
<h4> Slope of loss function with respect to weight </h4>
<p>
  Calculating the slope of the loss function for weights is the same as the method found in the last layer.
</p>
<p>
  (The calculations and code in this article are trial and error.)
</p>

  </div>
  <div class="bottom">
    <h3>Associated Information</h3>

<div style="margin:10px 0">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4525414114581084"
     crossorigin="anonymous"></script>
<ins class="adsbygoogle"
     style="display:block"
     data-ad-format="autorelaxed"
     data-ad-client="ca-pub-4525414114581084"
     data-ad-slot="9163995495"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
</div>

  </div>
</div>

        </div>
        <div class="side">
          <div style="margin-bottom:30px">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4525414114581084"
     crossorigin="anonymous"></script>
<!-- ディスプレイ縦長 --><ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4525414114581084"
     data-ad-slot="4544919209"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
</div>

<div style="margin-bottom:30px;padding:0 8px;">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4525414114581084"
     crossorigin="anonymous"></script>
<ins class="adsbygoogle"
     style="display:block"
     data-ad-format="autorelaxed"
     data-ad-client="ca-pub-4525414114581084"
     data-ad-slot="9163995495"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
</div>

        </div>
      </div>
      <div class="footer">
        <div class="perlri_link">
  <a rel="nofollow" href="https://perlclub.net/en">Perl Club</a>
</div>

      </div>
    </div>
  </body>
</html>
