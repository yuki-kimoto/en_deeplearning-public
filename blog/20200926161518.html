<!DOCTYPE html>
<html>
  <head>
    <!-- meta --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1.0,minimum-scale=1.0">
<link rel="icon" type="image/x-icon" href="/images/logo.png">
<link rel="stylesheet" type="text/css" href="/css/common.css">

<script type="text/javascript" src="/js/jquery-1.9.0.min.js"></script>
<script type="text/javascript" src="/js/google-code-prettify/prettify.js"></script>
<link  type="text/css" rel="stylesheet" href="/js/google-code-prettify/prettify.css"/>
<script>
  $(function(){
    $("pre").addClass("prettyprint");
    function init(event){
      prettyPrint();
    }
    if(window.addEventListener)window.addEventListener("load",init,false);
    else if(window.attachEvent)window.attachEvent("onload",init);
    
    $(".to-top").click(function() {
      $('body, html').animate({scrollTop: 0}, 300, 'linear');;
    });
  });
</script>

<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-5YWD0EFVYR"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-5YWD0EFVYR');
</script>

<title> MNIST handwriting recognition deep learning written in pure Perl  - Perl AI Deep Learning Tutorial</title>
<meta name="description" content="We will publish MNIST handwriting recognition deep learning written in pure Perl.">
  </head>
  <body>
    <div class="container">
      <div class="header">
        <div class="header_main">
  <h1>
    <a href="/"><img src="/images/logo.png">Perl AI Deep Learning Tutorial</a>
  </h1>
</div>

      </div>
      <div class="main">
        <div class="content">
          <div class="entry">
  <div class="top">
    <!-- top -->
  </div>
  <div class="middle">
    <h2><a href="/blog/20200926161518.html"> MNIST handwriting recognition deep learning written in pure Perl </a></h2>
<p>
  We will publish MNIST handwriting recognition deep learning written in pure Perl.
</p>
<div style="width:calc(100% - 30px);margin:10px auto;">
  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4525414114581084"
       crossorigin="anonymous"></script>
  <!-- 最初の段落下 - ディスプレイ 横長 レスポンシブ -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-4525414114581084"
       data-ad-slot="2828858335"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


<p>
  The MNIST handwritten data obtained from <a href="http://yann.lecun.com/exdb/mnist/"> THE MNIST DATABASE of handwritten digits</a> is placed under the data directory.
</p>
<p>
  This deep learning Perl code is based on the source code of <a href="https://nnadl-ja.github.io/nnadl_site_ja/index.html"> Neural Networks and Deep Learning</a>.
</p>
<p>
  As it is pure Perl code, it can be useful for Perl users to get an overview of deep learning algorithms.
</p>
<p>
  It doesn't use any special libraries, so if you have knowledge of if and for statements and arrays, you can read the source code. Since it consists of an if statement, a for statement, and an array, you can write deep learning logic in other programming languages ​​as a reference.
</p>
<p>
  Currently, this code does not have the performance of Perl's numerical calculation, so performance improvement is required.
</p>
<p>
  Partial rewrite to the C language transpiler <a href="https://metacpan.org/pod/SPVM/"> SPVM</a>, through trial and error to a realistic speed level It will be faster.
</p>
<pre>
# network.pl

use strict;
use warnings;
use FindBin;
use List::Util'shuffle';

# Learning rate
my $learning_rate = 3;

#Epoch Count-Number of Training Set Executions
my $epoch_count = 30;

# Mini batch size
my $mini_batch_size = 10;

#Number of neurons in each layer
# 28 * 28 = 728 monochrome image (input)
# Through 30 intermediate outputs (intermediate output)
Classify into 10 items from # 0 to 9 (output)
my $neurons_count_in_layers = [728, 30, 10];

#Information about the function that converts m inputs in each layer to n outputs. Number of inputs, number of outputs, bias, weight
my $m_to_n_func_infos = [];

#Create information on the conversion function from the number of neurons in each layer to m to n
for (my $i = 0; $i &lt;@$neurons_count_in_layers --1; $i ++) {
  my $inputs_length = $neurons_count_in_layers-&gt;[$i];
  my $outputs_length = $neurons_count_in_layers-&gt;[$i + 1];
  
  #Initialize bias with 0
  my $biases = array_new_zero ($outputs_length);
  
  Initialize the weight with the initial value of #Xivier. Weight is column priority matrix
  my $weights_mat = mat_new_zero ($outputs_length, $inputs_length);
  my $weights_length = $weights_mat-&gt;{rows_length} * $weights_mat-&gt;{columns_length};
  $weights_mat-&gt;{values} = array_create_he_init_value ($weights_length, $inputs_length);

  # Set conversion function information
  $m_to_n_func_infos-&gt;[$i] = {
    inputs_length =&gt; $inputs_length,
    outputs_length =&gt; $outputs_length,
    biases =&gt; $biases,
    weights_mat =&gt; $weights_mat,
  };;
}

#MNIEST Read image information-Handwritten training data used for input
my $mnist_train_image_file = "$FindBin::Bin / data / train-images-idx3-ubyte";
my $mnist_train_image_info = load_mnist_train_image_file ($mnist_train_image_file);

#Read MNIEST Label Information-Expected Output of Handwritten Training Data
my $mnist_train_label_file = "$FindBin::Bin / data / train-labels-idx1-ubyte";
my $mnist_train_label_info = load_mnist_train_label_file ($mnist_train_label_file);

#Training data index(only the first 40,000 are used as training data, and the remaining 10,000 are used as verification data)
my @training_data_indexes = (0 .. 40000);

# Information on each conversion function in mini-batch units
my $m_to_n_func_mini_batch_infos = [];

# Created by initializing the total of the bias slopes of each conversion function in the mini-batch and the weight slope of each conversion function in the mini-batch to 0.
#Initialize here to use memory area repeatedly
for (my $m_to_n_func_index = 0; $m_to_n_func_index &lt;@$m_to_n_func_infos; $m_to_n_func_index ++) {
  my $m_to_n_func_info = $m_to_n_func_infos-&gt;[$m_to_n_func_index];
  my $biases = $m_to_n_func_info-&gt;{biases};
  my $weights_mat = $m_to_n_func_info-&gt;{weights_mat};
  
  #Bias length
  my $biases_length = @$biases;
  
  # Created by initializing the sum of the bias slopes of each conversion function in a mini-batch to 0
  my $bias_grad_totals = array_new_zero ($biases_length);
  $m_to_n_func_mini_batch_infos-&gt;[$m_to_n_func_index] {biase_grad_totals} = $biase_grad_totals;

  # Created by initializing the sum of the slopes of the weights of each conversion function in the mini-batch to 0
  my $weight_grad_totals_mat = mat_new_zero ($weights_mat-&gt;{rows_length}, $weights_mat-&gt;{columns_length});
  $m_to_n_func_mini_batch_infos-&gt;[$m_to_n_func_index] {weight_grad_totals_mat} = $weight_grad_totals_mat;
}

# Total number of executions
my $total_count = 0;

# The number of correct answers
my $answer_match_count = 0;

#Run the training set as many times as you epoch
for (my $epoch_index = 0; $epoch_index &lt;$epoch_count; $epoch_index ++) {
  
  # Shuffle the index of training data (It seems that it is more generalized to train randomly)
  my @training_data_indexes_shuffle = shuffle @training_data_indexes;
  
  my $count = 0;
  
  #Learning in mini-batch size units
  my $backprop_count = 0;

  while (my @indexed_for_mini_batch = splice(@training_data_indexes_shuffle, 0, $mini_batch_size)) {
    
    # Initialize the sum of the bias slopes of each conversion function in the mini-batch and the sum of the weight slopes of each conversion function in the mini-batch to 0.
    for (my $m_to_n_func_index = 0; $m_to_n_func_index &lt;@$m_to_n_func_mini_batch_infos; $m_to_n_func_index ++) {
      my $m_to_n_func_info = $m_to_n_func_infos-&gt;[$m_to_n_func_index];
      my $biases = $m_to_n_func_info-&gt;{biases};
      my $weights_mat = $m_to_n_func_info-&gt;{weights_mat};
      
      # Created by initializing the sum of the bias slopes of each conversion function in a mini-batch to 0
      $m_to_n_func_mini_batch_infos-&gt;[$m_to_n_func_index] {biase_grad_totals} = array_new_zero (scalar @{$m_to_n_func_mini_batch_infos-&gt;[$m_to_n_func_index] {biase_grad_totals}})

      # Created by initializing the sum of the slopes of the weights of each conversion function in the mini-batch to 0
      $m_to_n_func_mini_batch_infos-&gt;[$m_to_n_func_index] {weight_grad_totals_mat} {values} = array_new_zero (scalar @{$m_to_n_func_mini_batch_infos-&gt;[$m_to_n_func_index] {weight_grad_total
    }
    
    for my $training_data_index (@indexed_for_mini_batch) {
      # Get the slope for the weight and bias loss function using the reverse mispropagation method
      my $m_to_n_func_grad_infos = backprop ($m_to_n_func_infos, $mnist_train_image_info, $mnist_train_label_info, $training_data_index);
      
      #Bias loss function slope
      my $bias_grads = $m_to_n_func_grad_infos-&gt;{biases};
      
      # Slope with respect to weight loss function
      my $weight_grads_mat = $m_to_n_func_grad_infos-&gt;{weights_mat};

      # Add the sum of the bias slopes of each conversion function in the mini-batch and the weight slope of each conversion function in the mini-batch.for (my $m_to_n_func_index = 0; $m_to_n_func_index &lt;@$m_to_n_func_mini_batch_infos; $m_to_n_func_index ++) {
        my $m_to_n_func_info = $m_to_n_func_infos-&gt;[$m_to_n_func_index];
        
        # Add the slope of the bias of each transmutation function in the mini-batch
        array_add_inplace ($m_to_n_func_mini_batch_infos-&gt;[$m_to_n_func_index] {biase_grad_totals}, $biase_grads-&gt;[$m_to_n_func_index]);

        # Add the slope of the weight of each transmutation function in the mini-batch
        array_add_inplace ($m_to_n_func_mini_batch_infos-&gt;[$m_to_n_func_index] {weight_grad_totals_mat} {values}, $weight_grads_mat-&gt;[$m_to_n_func_index] {values});
      }
    }

    # Update the bias and weight of each transform function using the sum of the slopes of the mini-batch
    for (my $m_to_n_func_index = 0; $m_to_n_func_index &lt;@$m_to_n_func_infos; $m_to_n_func_index ++) {
      
      #Update the bias of each transmutation function (consider the learning rate and divide by the number of mini-batch)
      update_params ($m_to_n_func_infos-&gt;[$m_to_n_func_index] {biases}, $m_to_n_func_mini_batch_infos-&gt;[$m_to_n_func_index] {biase_grad_totals}, $learning_rate, $mini_batch_size);
      
      #Update the weight of each transmutation function (consider the learning rate, divide the total slope by the number of mini-batch, divide by the number of mini-batch)
      update_params ($m_to_n_func_infos-&gt;[$m_to_n_func_index] {weights_mat} {values}, $m_to_n_func_mini_batch_infos-&gt;[$m_to_n_func_index] {weight_grad_totals_mat} {values}, $learning_rate
    }
  }
}

#Update parameters considering learning rate and number of mini-batch
sub update_params {
  my ($params, $param_grads, $learning_rate, $mini_batch_size) = @_;
  
  for (my $param_index = 0; $param_index &lt;@$params; $param_index ++) {
    my $update_diff = ($learning_rate / $mini_batch_size) * $param_grads-&gt;[$param_index];
    $params-&gt;[$param_index]-= $update_diff;
  }
}

my $count = 0;

# Reverse mispropagation method
sub backprop {
  my ($m_to_n_func_infos, $mnist_train_image_info, $mnist_train_label_info, $training_data_index) = @_;
  
  my $first_inputs_length = $m_to_n_func_infos-&gt;[0] {inputs_length};
  
  # Input (convert 0 to 255 values ​​to 0 to 1)
  my $image_unit_length = $mnist_train_image_info-&gt;{rows_count} * $mnist_train_image_info-&gt;{columns_count};
  my $mnist_train_image_data = $mnist_train_image_info-&gt;{data};
  my $first_inputs_packed = substr($mnist_train_image_data, $image_unit_length * $training_data_index, $image_unit_length);
  my $first_inputs_raw = [unpack("C $first_inputs_length", $first_inputs_packed)];
  my $first_inputs = array_div_scalar ($first_inputs_raw, 255);
  
  # Probability distribution of expected output
  my $label_number = $mnist_train_label_info-&gt;{label_numbers} [$training_data_index];
  
  my $desired_outputs = probabilize_desired_outputs ($label_number);
  
  #Bias slope of each transmutation function
  my $bias_grads_in_m_to_n_funcs = [];
  
  # Slope of weight of each transmutation function
  my $weight_grads_mat_in_m_to_n_funcs = [];
  
  #Initialization of bias slope and weight slope
  for (my $m_to_n_func_index = 0; $m_to_n_func_index &lt;@$m_to_n_func_infos; $m_to_n_func_index ++) {
    my $inputs_length = $m_to_n_func_infos-&gt;[$m_to_n_func_index] {inputs_length};
    my $outputs_length = $m_to_n_func_infos-&gt;[$m_to_n_func_index] {outputs_length};

    #Initialize bias slope to 0
    $bias_grads_in_m_to_n_funcs-&gt;[$m_to_n_func_index] = array_new_zero ($outputs_length);

    #Initialize the slope of the weight with 0
    $weight_grads_mat_in_m_to_n_funcs-&gt;[$m_to_n_func_index] = mat_new_zero ($outputs_length, $inputs_length);
  }

  # Input for each layer
  my $inputs_in_m_to_n_funcs = [$first_inputs];
  
  #Activated output of each layer
  my $outputs_in_m_to_n_funcs = [];
  
  # Obtain the output of the output layer from the input of the input layer
  # Save the output of each layer and the activated output due to the reverse mispropagation method
  for (my $m_to_n_func_index = 0; $m_to_n_func_index &lt;@$m_to_n_func_infos; $m_to_n_func_index ++) {
    my $cur_inputs = $inputs_in_m_to_n_funcs-&gt;[-1];
    my $inputs_length = $m_to_n_func_infos-&gt;[$m_to_n_func_index] {inputs_length};
    my $outputs_length = $m_to_n_func_infos-&gt;[$m_to_n_func_index] {outputs_length};
    
    #Weight matrix
    my $weights_mat = $m_to_n_func_infos-&gt;[$m_to_n_func_index] {weights_mat};
    
    #Input matrix
    my $cur_inputs_rows_length = $outputs_length;
    my $cur_inputs_columns_length = 1;
    my $cur_inputs_mat = {
      rows_length =&gt; $cur_inputs_rows_length,
      columns_length =&gt; $cur_inputs_columns_length,
      values ​​=&gt; $cur_inputs,
    };;
    
    # Matrix product of weights and inputs
    my $mul_weights_inputs_mat = mat_mul ($weights_mat, $cur_inputs_mat);
    my $mul_weights_inputs = $mul_weights_inputs_mat-&gt;{values};
    
    #Bias
    my $biases = $m_to_n_func_infos-&gt;[$m_to_n_func_index] {biases};
    
    #Output-The sum of the weight and input matrix product and bias
    my $outputs = array_add ($mul_weights_inputs, $biases);
    
    # Activated output-Apply activation function to output
    my $activate_outputs = array_sigmoid ($outputs);
    
    #Save output for reverse mispropagation
    push @$outputs_in_m_to_n_funcs, $outputs;
    
    # Save the following input for backflip method
    push @$inputs_in_m_to_n_funcs, $activate_outputs;
  }

  
  #Last output
  my $last_outputs = $outputs_in_m_to_n_funcs-&gt;[-1];
  
  #Last activated output
  my $last_activate_outputs = pop @$inputs_in_m_to_n_funcs;
  
  #softmax function
  # my $softmax_outputs = softmax ($last_activate_outputs);
  
  #Error
  my $cost = cross_entropy_cost ($last_activate_outputs, $desired_outputs);
  print "Cost:". Sprintf ("%.3f", $cost). "\ n";
  
  # Whether you answered correctly
  my $answer = max_index ($last_activate_outputs);
  # my $answer = max_index ($softmax_outputs);
  my $desired_answer = max_index ($desire

  </div>
  <div class="bottom">
    <h3>Associated Information</h3>

<div style="margin:10px 0">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4525414114581084"
     crossorigin="anonymous"></script>
<ins class="adsbygoogle"
     style="display:block"
     data-ad-format="autorelaxed"
     data-ad-client="ca-pub-4525414114581084"
     data-ad-slot="9163995495"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
</div>

  </div>
</div>

        </div>
        <div class="side">
          <div style="margin-bottom:30px">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4525414114581084"
     crossorigin="anonymous"></script>
<!-- ディスプレイ縦長 --><ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4525414114581084"
     data-ad-slot="4544919209"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
</div>

<div style="margin-bottom:30px;padding:0 8px;">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4525414114581084"
     crossorigin="anonymous"></script>
<ins class="adsbygoogle"
     style="display:block"
     data-ad-format="autorelaxed"
     data-ad-client="ca-pub-4525414114581084"
     data-ad-slot="9163995495"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
</div>

        </div>
      </div>
      <div class="footer">
        <div class="perlri_link">
  <a rel="nofollow" href="https://perlclub.net/en">Perl Club</a>
</div>

      </div>
    </div>
  </body>
</html>
