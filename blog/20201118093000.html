<!DOCTYPE html>
<html>
  <head>
    <!-- Google Automatic Advertising -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4525414114581084"
     crossorigin="anonymous"></script>

<!-- meta --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1.0,minimum-scale=1.0">
<link rel="icon" type="image/x-icon" href="/images/logo.png">
<link rel="stylesheet" type="text/css" href="/css/common.css">

<script type="text/javascript" src="/js/jquery-1.9.0.min.js"></script>
<script type="text/javascript" src="/js/google-code-prettify/prettify.js"></script>
<link  type="text/css" rel="stylesheet" href="/js/google-code-prettify/prettify.css"/>
<script>
  $(function(){
    $("pre").addClass("prettyprint");
    function init(event){
      prettyPrint();
    }
    if(window.addEventListener)window.addEventListener("load",init,false);
    else if(window.attachEvent)window.attachEvent("onload",init);
    
    $(".to-top").click(function() {
      $('body, html').animate({scrollTop: 0}, 300, 'linear');;
    });
  });
</script>

<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-5YWD0EFVYR"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-5YWD0EFVYR');
</script>

<title> Perl's Deep Learning Library-AI::MXNet  - Perl AI Deep Learning Tutorial</title>
<meta name="description" content="Perl's deep learning library includes  AI::MXNet. It is a library of deep learning written in C ++ that can be used by binding it with Perl.">
  </head>
  <body>
    <div class="container">
      <div class="header">
        <div class="header_main">
  <h1>
    <a href="/"><img src="/images/logo.png">Perl AI Deep Learning Tutorial</a>
  </h1>
</div>

      </div>
      <div class="main">
        <div class="content">
          <div class="entry">
  <div class="top">
    <!-- top -->
  </div>
  <div class="middle">
    <h2><a href="/blog/20201118093000.html"> Perl's Deep Learning Library-AI::MXNet </a></h2>
<p>
  Perl's deep learning library includes <a href="https://metacpan.org/pod/AI ::MXNet"> AI::MXNet</a>. It is a library of deep learning written in C ++ that can be used by binding it with Perl.
</p>
<div style="width:calc(100% - 30px);margin:10px auto;">
  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4525414114581084"
       crossorigin="anonymous"></script>
  <!-- 最初の段落下 - ディスプレイ 横長 レスポンシブ -->
  <ins class="adsbygoogle"
       style="display:block"
       data-ad-client="ca-pub-4525414114581084"
       data-ad-slot="2828858335"
       data-ad-format="auto"
       data-full-width-responsive="true"></ins>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({});
  </script>
</div>


<p>
  First of all, if you want to actually try deep learning, you can easily (?) Use the library. Deep learning is also known as deep learning. If you're looking for a Perl library for deep learning, AI::MXNet is for you.
</p>
<h3> Official support for MXNet on Amazon AWS </h3>
<p>
  MXNet has official support on Amazon AWS.
</p>
<blockquote>
<p>
  Flexibility and choice
</p>
<p>
  <b> MXNet </b> supports a wide range of programming languages ​​such as C ++, JavaScript, Python, R, Matlab, Julia, Scala, Clojure, and <b> Perl </b>, so you already know it. You can start in the language you are in. However, the backend compiles all the code into C ++ for maximum performance regardless of the language used to build the model.
</p>
<p>
  <a href="https://aws.amazon.com/jp/mxnet/"> Apache MXNet on AWS</a>
</p>
</blockquote>
<p>
  AI::MXNet is one of the few Amazon Perl supports ...
</p>
<h3> Sample of image generation using deep learning </h3>
<p>
  According to the English blog of AI::MXNet author Sergey V. Kolychev, it seems that you can also generate images using deep learning. The author himself seems to be doing deep learning related to natural language processing in his work.
</p>
<blockquote>
<p>
  I hope you enjoy this example and create a lot of nice pictures. Below is an image generated by a sample script made from a classic painting that is different from the photo of Kyubi.
</p>
<p>
  <a href="http://blogs.perl.org/users/sergey_kolychev/2018/07/machine-learning-in-perl-kyuubi-goes-to-a-modelzoo-during-the-starry-night.html "> Machine learning in Perl: Kyuubi goes to a (Model) Zoo during The Starry Night.</a>
</p>
</blockquote>
<h4> Original image </h4>
<p>
  <img src = "/ images / aimxnet / kyuubi.jpg" width = "500">
</p>
<h4> Deep learning generated image </h4>
<p>
  Is it something that was generated from the original image by learning something like Van Gogh style painting?
</p>
<p>
  <img src = "/ images / aimxnet / kyuubi_blacksquare.jpg">
</p>
<p>
  <img src = "/ images / aimxnet / kyuubi_dali.jpg">
</p>
<p>
  <img src = "/ images / aimxnet / kyuubi_mural.jpg">
</p>
<p>
  <img src = "/ images / aimxnet / kyuubi_starry.jpg">
</p>
<h3> How to use AI::MXNet </h3>
<p>
  Here's how to use it from a sample.
</p>
<pre>
## Convolutional NN for recognizing hand-written digits in MNIST dataset
## It's considered "Hello, World" for Neural Networks
## For more info about the MNIST problem please refer to L &lt;http://neuralnetworksanddeeplearning.com/chap1.html&gt;
 
use strict;
use warnings;
use AI::MXNet qw (mx);
use AI::MXNet::TestUtils qw (GetMNIST_ubyte);
use Test::More tests =&gt; 1;
 
# symbol net
my $batch_size = 100;
 
### model
my $data = mx-&gt;symbol-&gt;Variable('data');
my $conv1 = mx-&gt;symbol-&gt;Convolution(data =&gt; $data, name =&gt;'conv1', num_filter =&gt; 32, kernel =&gt; [3,3], stride =&gt; [2,2]);
my $bn1 = mx-&gt;symbol-&gt;BatchNorm(data =&gt; $conv1, name =&gt; "bn1");
my $act1 = mx-&gt;symbol-&gt;Activation(data =&gt; $bn1, name =&gt;'relu1', act_type =&gt; "relu");
my $mp1 = mx-&gt;symbol-&gt;Pooling(data =&gt; $act1, name =&gt;'mp1', kernel =&gt; [2,2], stride =&gt; [2,2], pool_type =&gt;'max') ;
 
my $conv2 = mx-&gt;symbol-&gt;Convolution(data =&gt; $mp1, name =&gt;'conv2', num_filter =&gt; 32, kernel =&gt; [3,3], stride =&gt; [2,2]);
my $bn2 = mx-&gt;symbol-&gt;BatchNorm(data =&gt; $conv2, name =&gt; "bn2");
my $act2 = mx-&gt;symbol-&gt;Activation(data =&gt; $bn2, name =&gt;'relu2', act_type =&gt; "relu");
my $mp2 = mx-&gt;symbol-&gt;Pooling(data =&gt; $act2, name =&gt;'mp2', kernel =&gt; [2,2], stride =&gt; [2,2], pool_type =&gt;'max') ;
 
 
my $fl = mx-&gt;symbol-&gt;Flatten(data =&gt; $mp2, name =&gt; "flatten");
my $fc1 = mx-&gt;symbol-&gt;FullyConnected(data =&gt; $fl, name =&gt; "fc1", num_hidden =&gt; 30);
my $act3 = mx-&gt;symbol-&gt;Activation(data =&gt; $fc1, name =&gt;'relu3', act_type =&gt; "relu");
my $fc2 = mx-&gt;symbol-&gt;FullyConnected(data =&gt; $act3, name =&gt;'fc2', num_hidden =&gt; 10);
my $softmax = mx-&gt;symbol-&gt;SoftmaxOutput(data =&gt; $fc2, name =&gt;'softmax');
 
# check data
GetMNIST_ubyte ();
 
my $train_dataiter = mx-&gt;io-&gt;MNISTIter({{
    image =&gt; "data / train-images-idx3-ubyte",
    label =&gt; "data / train-labels-idx1-ubyte",
    data_shape =&gt; [1, 28, 28],
    batch_size =&gt; $batch_size, shuffle =&gt; 1, flat =&gt; 0, silent =&gt; 0, seed =&gt; 10});
my $val_dataiter = mx-&gt;io-&gt;MNISTIter({{
    image =&gt; "data / t10k-images-idx3-ubyte",
    label =&gt; "data / t10k-labels-idx1-ubyte",
    data_shape =&gt; [1, 28, 28],
    batch_size =&gt; $batch_size, shuffle =&gt; 1, flat =&gt; 0, silent =&gt; 0});
 
my $n_epoch = 1;
my $mod = mx-&gt;mod-&gt;new(symbol =&gt; $softmax);
$mod-&gt;fit(
    $train_dataiter,
    eval_data =&gt; $val_dataiter,
    optimizer_params =&gt; {learning_rate =&gt; 0.01, momentum =&gt; 0.9},
    num_epoch =&gt; $n_epoch
);
my $res = $mod-&gt;score($val_dataiter, mx-&gt;metric-&gt;create('acc'));
ok ($res-&gt;{accuracy}&gt; 0.8);
 
## Gluon MNIST example
 
my $net = nn-&gt;Sequential();
$net-&gt;name_scope(sub {
    $net-&gt;add(nn-&gt;Dense(128, activation =&gt;'relu'));
    $net-&gt;add(nn-&gt;Dense(64, activation =&gt;'relu'));
    $net-&gt;add(nn-&gt;Dense(10));
});
$net-&gt;hybridize;
 
# data
sub transformer
{
    my ($data, $label) = @_;
    $data = $data-&gt;reshape([-1])-&gt;astype('float32') / 255;
    return($data, $label);
}
my $train_data = gluon-&gt;data-&gt;DataLoader(
    gluon-&gt;data-&gt;vision-&gt;MNIST('./ data', train =&gt; 1, transform =&gt; \ &amp; transformer),
    batch_size =&gt; $batch_size, shuffle =&gt; 1, last_batch =&gt;'discard'
);
 
## training
sub train
{
    my ($epochs, $ctx) = @_;
    # Collect all parameters from net and its children, then initialize them.
    $net-&gt;initialize(mx-&gt;init-&gt;Xavier(magnitude =&gt; 2.24), ctx =&gt; $ctx);
    #Trainer is for updating parameters with gradient.
    my $trainer = gluon-&gt;Trainer($net-&gt;collect_params(),'sgd', {learning_rate =&gt; $lr, momentum =&gt; $momentum});
    my $metric = mx-&gt;metric-&gt;Accuracy();
    my $loss = gluon-&gt;loss-&gt;SoftmaxCrossEntropyLoss();for my $epoch (0 .. $epochs-1)
    {
        # reset data iterator and metric at begining of epoch.
        $metric-&gt;reset();
        enumerate (sub {
            my ($i, $d) = @_;
            my ($data, $label) = @$d;
            $data = $data-&gt;as_in_context($ctx);
            $label = $label-&gt;as_in_context($ctx);
            # Start recording computation graph with record () section.
            #Recorded graphs can then be differentiated with backward.
            my $output;
            autograd-&gt;record(sub {
                $output = $net-&gt;($data);
                my $L = $loss-&gt;($output, $label);
                $L-&gt;backward;
            });
            # take a gradient step with batch_size equal to data.shape [0]
            $trainer-&gt;step($data-&gt;shape-&gt;[0]);
            #update metric at last.
            $metric-&gt;update([$label], [$output]);
 
            if ($i%$log_interval == 0 and $i&gt; 0)
            {
                my ($name, $acc) = $metric-&gt;get();
                print "[Epoch $epoch Batch $i] Training: $name = $acc\n";
            }
        }, \ @{$train_data});
 
        my ($name, $acc) = $metric-&gt;get();
        print "[Epoch $epoch] Training: $name = $acc\n";
 
        my ($val_name, $val_acc) = test ($ctx);
        print "[Epoch $epoch] Validation: $val_name = $val_acc\n"
    }
    $net-&gt;save_parameters('mnist.params');
}
 
train ($epochs, $cuda? mx-&gt;gpu(0): mx-&gt;cpu);
</pre>

  </div>
  <div class="bottom">
    <h3>Associated Information</h3>

<div style="margin:10px 0">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4525414114581084"
     crossorigin="anonymous"></script>
<ins class="adsbygoogle"
     style="display:block"
     data-ad-format="autorelaxed"
     data-ad-client="ca-pub-4525414114581084"
     data-ad-slot="9163995495"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
</div>

  </div>
</div>

        </div>
        <div class="side">
          
        </div>
      </div>
      <div class="footer">
        <div class="perlri_link">
  <a rel="nofollow" href="https://perlclub.net/en">Perl Club</a>
</div>

      </div>
    </div>
  </body>
</html>
